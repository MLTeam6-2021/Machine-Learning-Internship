{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "joyce_scraper.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CUXYoVgnS0jP"
      ],
      "authorship_tag": "ABX9TyPXYCzeEyqxvT+6lC8/EoF0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKrc_Yx7hhw4"
      },
      "source": [
        "# Webscraping Tapas Forum "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckh6gyttZBiW"
      },
      "source": [
        "''' instructions: run main() to get the json, pickle, and csv files with the latest 1080 posts \n",
        "    note: files that are stored on colab notebook disappear when runtime disconnects. save files immediately. ''' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azx5DOB0hs7-"
      },
      "source": [
        "!pip install beautifulsoup4 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V2ivo-VCOU6"
      },
      "source": [
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7tBIFCnCbj6"
      },
      "source": [
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('-headless')\n",
        "options.add_argument('-no-sandbox')\n",
        "options.add_argument('-disable-dev-shm-usage')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8jXgNdahFap"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-jcxE5eCjkK"
      },
      "source": [
        "def get_source_code(url): \n",
        "  wd = webdriver.Chrome('chromedriver', options=options)\n",
        "  wd.get(url)\n",
        "  source = wd.page_source\n",
        "  return source"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxC0oTyx6LrQ"
      },
      "source": [
        "# making the soup \n",
        "def make_soup(url):\n",
        "  page = requests.get(url, timeout=120)\n",
        "  if page.status_code == 200:\n",
        "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "    return soup\n",
        "  else: \n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xee5f8mO7Y7Z"
      },
      "source": [
        "def special_convert_str_to_int(string): \n",
        "  if string[-1] == 'k': \n",
        "    float_string = float(string[:-1]) \n",
        "    final_string = int(float_string * 1000)\n",
        "  else: \n",
        "    final_string = int(string)\n",
        "  return final_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln2k7PLn5r5z"
      },
      "source": [
        "# get link, href, title, replies, views data (located in a cell/column) on one post (located in a row)\n",
        "def get_table_cells_data(table_row): \n",
        "  cell_data = {}\n",
        "  link, href, title, replies, views = \"\", \"\", \"\", \"\", \"\"\n",
        "  base_url = \"https://forums.tapas.io\"\n",
        "  td = table_row.find_all(\"td\")\n",
        "  for cell in td: \n",
        "    # print(\"cell class: \", cell[\"class\"])\n",
        "    if cell[\"class\"] == [\"main-link\", \"clearfix\"]: \n",
        "      href = cell.a[\"href\"]\n",
        "      link = base_url + href \n",
        "      title = cell.a.text\n",
        "      cell_data[\"href\"] = href\n",
        "      cell_data[\"link\"] = link \n",
        "      cell_data[\"title\"] = title\n",
        "    elif cell[\"class\"] == \"posters\": \n",
        "      continue\n",
        "    elif (cell[\"class\"]==['num', 'posts-map', 'posts', 'heatmap-high']) or (cell[\"class\"]==['num', 'posts-map', 'posts', 'heatmap-']) or (cell[\"class\"]==['num', 'posts-map', 'posts', 'heatmap-med']): \n",
        "      replies = cell.a.span.text\n",
        "      num_replies = special_convert_str_to_int(replies)\n",
        "      cell_data[\"replies\"] = num_replies\n",
        "    elif (cell[\"class\"]==['num', 'views', '']) or (cell[\"class\"]==['num', 'views', 'heatmap-med']) or (cell[\"class\"]==['num', 'views', 'heatmap-high']):\n",
        "      views = cell.span.text\n",
        "      num_views = special_convert_str_to_int(views) \n",
        "      cell_data[\"views\"] = num_views\n",
        "    elif (cell[\"class\"] == ['num', 'age', 'activity']) or (cell[\"class\"] == ['num', 'age', 'coldmap-low', 'activity']):\n",
        "      continue \n",
        "  '''\n",
        "  print(\"href: \", href)\n",
        "  print(\"link: \", link)\n",
        "  print(\"title: \", title)\n",
        "  print(\"replies: \", replies)\n",
        "  print(\"views: \", views)\n",
        "  '''\n",
        "  return cell_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMptmK9NhlQ1"
      },
      "source": [
        "# get table rows data (link, href, post_id, replies, views) for one page (30 posts)\n",
        "def get_table_rows_data(url):\n",
        "  \n",
        "  source_code = get_source_code(url)\n",
        "  soup = BeautifulSoup(source_code, \"html.parser\")\n",
        "  tables = soup.find_all(\"table\")\n",
        "  print(\"Number of Tables: \", (len(tables)))\n",
        "\n",
        "  post_data_lst = [] \n",
        "  post_id = \"\"\n",
        "  tbody = tables[0].tbody\n",
        "  table_rows = tbody.find_all(\"tr\")\n",
        "\n",
        "  for row in table_rows: \n",
        "    post_data = {}\n",
        "    post_id = row[\"data-topic-id\"]\n",
        "    post_data[\"post_id\"] = post_id \n",
        "    ''' print(\"post_id: \", post_id) ''' \n",
        "    extra_post_data = get_table_cells_data(row)\n",
        "    if extra_post_data[\"href\"] == \"\":  # if cannot get href do not add to list\n",
        "      continue \n",
        "    post_data.update(extra_post_data)\n",
        "    post_data_lst.append(post_data) \n",
        "\n",
        "  return post_data_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3ELlWbTz0Vw"
      },
      "source": [
        "# gets posts links, hrefs, titles, post_ids, replies, views for x number of pages. returns list of dictionaries. str & int parameters\n",
        "def get_all_meta_data(url, num_of_pages): \n",
        "  all_meta = []\n",
        "  latest_page = \"/l/latest?page=\"\n",
        "\n",
        "  # just scrape 30 * 36 = 1080 data points for analysis; Dont want whole forum \n",
        "  if num_of_pages > 36: \n",
        "    num_of_pages = 36\n",
        "  \n",
        "  for i in range(num_of_pages):\n",
        "    page_num = \"% s\" %i\n",
        "    print(\"page_num: \", page_num)\n",
        "    new_url = url + latest_page + page_num\n",
        "    print(\"new_url: \", new_url)\n",
        "    next_lst = get_table_rows_data(new_url)\n",
        "    if next_lst == []: \n",
        "      continue \n",
        "    all_meta.extend(next_lst)\n",
        "\n",
        "  print(len(all_meta)) \n",
        "  return all_meta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57F85LsCKkyG"
      },
      "source": [
        "def get_date_published(soup): \n",
        "  date = \"\"\n",
        "  date_data = soup.find(\"time\")\n",
        "  date = date_data['datetime']\n",
        "  end = date.find(\"T\") \n",
        "  date = date[:end]\n",
        "  return date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtiJptR8LFAs"
      },
      "source": [
        "def get_post_body(soup):\n",
        "  post = \"\"\n",
        "  original_post = soup.find(\"div\", {\"class\":\"post\"})\n",
        "  lines = original_post.find_all(\"p\")\n",
        "  for line in lines: \n",
        "    post = post + line.text\n",
        "  return post\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0A-CLfoh53Z"
      },
      "source": [
        "# adds date and post body to list of posts dictionaries \n",
        "def add_post_date_and_body(posts):  \n",
        "  count = 0 \n",
        "  for post in posts: \n",
        "    url = post[\"link\"]\n",
        "    soup = make_soup(url) \n",
        "    if soup == None: \n",
        "      continue \n",
        "    date = get_date_published(soup)\n",
        "    content = get_post_body(soup)\n",
        "    post[\"date\"] = date\n",
        "    post[\"post_content\"] = content\n",
        "    count += 1\n",
        "    print(count) \n",
        "    print(url)\n",
        "    time.sleep(3)\n",
        "  \n",
        "  return posts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gAhmo1EeerU"
      },
      "source": [
        "def scrape_category(url, pages):\n",
        "  # just scrape 3 * 36 = 1080 data points. dont want whole forum. thats too much \n",
        "  if pages > 36: \n",
        "    pages = 36 \n",
        "  \n",
        "  category_posts_lst = get_all_meta_data(url, pages) \n",
        "  print(\"===== Scraping Post Content ==================================================================================================\")\n",
        "  final_lst = add_post_date_and_body(category_posts_lst)\n",
        "  return final_lst "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgmaDY4mE1bf"
      },
      "source": [
        "def clean_data(posts_lst):\n",
        "  clean_df = pd.DataFrame()\n",
        "\n",
        "  # create dataframe from list of posts data\n",
        "  posts_df = pd.DataFrame(posts_lst)\n",
        "  posts_df.rename(columns={\"date\":\"date_published\"}, inplace=True)\n",
        "  \n",
        "  # change values of date published to datetime.date() object \n",
        "  posts_df[\"date_published\"] = pd.to_datetime(posts_df[\"date_published\"]).dt.date\n",
        "  \n",
        "  # sort rows by date published (most recent to oldest)\n",
        "  posts_sorted_df = posts_df.sort_values(by=\"date_published\", ascending=False, inplace=False) \n",
        "  \n",
        "  # delete duplicate rows \n",
        "  posts_sorted_clean_df = posts_sorted_df.drop_duplicates(keep='first')\n",
        "  \n",
        "  # reorder indices by date order (most recent to oldest)\n",
        "  date_index_df = posts_sorted_clean_df.set_index(\"date_published\", inplace=False)\n",
        "  clean_df = date_index_df.reset_index()\n",
        "\n",
        "  return clean_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gxNClzvB01b"
      },
      "source": [
        "base_url = \"https://forums.tapas.io\"\n",
        "latest_page = \"/l/latest?page=\"\n",
        "url_categories = {\"event\":{\"url\":\"https://forums.tapas.io/c/events-challenges\",\"last_page\":25,\"number_of_posts\":754},\n",
        "                  \"announcements\":{\"url\":\"https://forums.tapas.io/c/announcements\",\"last_page\":4,\"number_of_posts\":119},\n",
        "                  \"off_topic\":{\"url\":\"https://forums.tapas.io/c/Off-Topic\",\"last_page\":202, \"number_of_posts\":6076},\n",
        "                  \"art_comics\":{\"url\":\"https://forums.tapas.io/c/art-comics\",\"last_page\":265,\"number_of_posts\":7974},\n",
        "                  \"writing_novels\":{\"url\":\"https://forums.tapas.io/c/writing-novels\",\"last_page\":44, \"number_of_posts\":1305},\n",
        "                  \"reviews\":{\"url\":\"https://forums.tapas.io/c/reviews-feedback\",\"last_page\":25,\"number_of_posts\":755},\n",
        "                  \"collaborations\":{\"url\":\"https://forums.tapas.io/c/collaborations\",\"last_page\":94,\"number_of_posts\":2844},\n",
        "                  \"questions\":{\"url\":\"https://forums.tapas.io/c/questions\",\"last_page\":275,\"number_of_posts\":8266}, \n",
        "                  \"answered\":{\"url\":\"https://forums.tapas.io/c/answered\",\"last_page\":14,\"number_of_posts\":421}, \n",
        "                  \"tech_support\":{\"url\":\"https://forums.tapas.io/c/tech-support-site-feedback\",\"last_page\":49,\"number_of_posts\":1474},\n",
        "                  \"promotions\":{\"url\":\"https://forums.tapas.io/c/promotions\",\"last_page\":191,\"number_of_posts\":5734}}\n",
        "url_categories_lst = [{\"category\":\"event\",\"url\":\"https://forums.tapas.io/c/events-challenges\",\"last_page\":25,\"number_of_posts\":754},\n",
        "                      {\"category\":\"announcements\",\"url\":\"https://forums.tapas.io/c/announcements\",\"last_page\":4,\"number_of_posts\":119},\n",
        "                      {\"category\":\"off_topic\",\"url\":\"https://forums.tapas.io/c/Off-Topic\",\"last_page\":202, \"number_of_posts\":6076},\n",
        "                      {\"category\":\"art_comics\",\"url\":\"https://forums.tapas.io/c/art-comics\",\"last_page\":265,\"number_of_posts\":7974},\n",
        "                      {\"category\":\"writing_novels\",\"url\":\"https://forums.tapas.io/c/writing-novels\",\"last_page\":44, \"number_of_posts\":1305},\n",
        "                      {\"category\":\"reviews\",\"url\":\"https://forums.tapas.io/c/reviews-feedback\",\"last_page\":25,\"number_of_posts\":755},\n",
        "                      {\"category\":\"collaborations\",\"url\":\"https://forums.tapas.io/c/collaborations\",\"last_page\":94,\"number_of_posts\":2844},\n",
        "                      {\"category\":\"questions\",\"url\":\"https://forums.tapas.io/c/questions\",\"last_page\":275,\"number_of_posts\":8266}, \n",
        "                      {\"category\":\"answered\",\"url\":\"https://forums.tapas.io/c/answered\",\"last_page\":14,\"number_of_posts\":421}, \n",
        "                      {\"category\":\"tech_support\",\"url\":\"https://forums.tapas.io/c/tech-support-site-feedback\",\"last_page\":49,\"number_of_posts\":1474},\n",
        "                      {\"category\":\"promotions\",\"url\":\"https://forums.tapas.io/c/promotions\",\"last_page\":191,\"number_of_posts\":5734}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcxgX_hXETVC"
      },
      "source": [
        "def main(): \n",
        "  for category, info in url_categories.items(): \n",
        "    print(\"=== Now Scraping \" + category + \" category =========================================================================================\")\n",
        "\n",
        "    # page numbering starts from 0. so total number of pages is last_page+1\n",
        "    category_posts = scrape_category(info[\"url\"], info[\"last_page\"]+1) \n",
        "\n",
        "    cat_json = \"tapas_\" + category + \"_first_1080_posts_2021.json\"\n",
        "    with open(cat_json,\"w\") as cat_write_file: \n",
        "      json.dump(category_posts, cat_write_file)    \n",
        "    with open(cat_json, \"r\") as cat_read_file: \n",
        "      category_posts_lst = json.load(cat_read_file) \n",
        "\n",
        "    cat_clean_df = clean_data(category_posts_lst)\n",
        "    \n",
        "    cat_pkl = \"tapas_\" + category + \"_first_1080_pickle.pkl\"\n",
        "    cat_clean_df.to_pickle(cat_pkl)\n",
        "\n",
        "    cat_csv = \"tapas_\" + category + \"_first_1080_posts_2021.csv\"\n",
        "    cat_clean_df.to_csv(cat_csv,index=False)    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g56V1DFS4VQJ"
      },
      "source": [
        "# scrape as many categories as you want from anywhere in the list\n",
        "def scrape_multiple_categories(url_categories_lst, start, stop):\n",
        "  for i in range(start, stop): \n",
        "    cat = url_categories_lst[i]\n",
        "    cat_name = cat[\"category\"] \n",
        "\n",
        "    # page numbering starts from 0 so total number of pages is last_page+1\n",
        "    category_posts = scrape_category(cat[\"url\"], cat[\"last_page\"]+1)\n",
        "\n",
        "    cat_json = \"tapas_\" + cat_name + \"_first_1080_posts_2021.json\"\n",
        "    with open(cat_json,\"w\") as cat_write_file: \n",
        "      json.dump(category_posts, cat_write_file)    \n",
        "    with open(cat_json, \"r\") as cat_read_file: \n",
        "      category_posts_lst = json.load(cat_read_file) \n",
        "\n",
        "    cat_clean_df = clean_data(category_posts_lst)\n",
        "    \n",
        "    cat_pkl = \"tapas_\" + cat_name + \"_first_1080_pickle.pkl\"\n",
        "    cat_clean_df.to_pickle(cat_pkl)\n",
        "\n",
        "    cat_csv = \"tapas_\" + cat_name + \"_first_1080_posts_2021.csv\"\n",
        "    cat_clean_df.to_csv(cat_csv,index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVF1pPwf3Ya0"
      },
      "source": [
        "# preventing google colab from disconnecting *works for about 7 hrs\n",
        "# inspect page, go to console, enter code separately \n",
        "'''\n",
        "function ClickConnect(){\n",
        "    console.log(\"Clicked on connect button\"); \n",
        "    document.querySelector(\"colab-connect-button\").click()\n",
        "}\n",
        "'''\n",
        "'''\n",
        "setInterval(ClickConnect,60000)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}